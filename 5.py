import os
import sys
import subprocess
import torch
from transformers import AutoTokenizer

# ================== Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯Ù†ÛŒØ§Ø² ==================
for pkg in ["torch", "transformers", "safetensors"]:
    subprocess.run([sys.executable, "-m", "pip", "install", "--upgrade", pkg])

# ================== Ú©Ù„ÙˆÙ† Ú©Ø±Ø¯Ù† Ù…Ø®Ø²Ù† ==================
REPO_URL = "https://github.com/BKHMSI/mixture-of-cognitive-reasoners.git"
PROJECT_PATH = os.path.join(os.getcwd(), "mixture-of-cognitive-reasoners")

if not os.path.exists(PROJECT_PATH):
    print("ğŸ“¥ Cloning Mixture-of-Cognitive-Reasoners repository ...")
    subprocess.run(["git", "clone", REPO_URL, PROJECT_PATH])
else:
    print("âœ… Repo already exists, skipping clone.")

# ================== Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ø¨Ù‡ sys.path ==================
if PROJECT_PATH not in sys.path:
    sys.path.append(PROJECT_PATH)

# ================== ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ù„ ==================
try:
    from mixture_of_cognitive_reasoners.models.micro_llama import MiCRoLlama, MiCRoLlamaConfig
except ModuleNotFoundError:
    print("âŒ Couldn't import MiCRoLlama â€” check repo path!")
    raise

# ================== Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ ==================
MODEL_ID = "bkhmsi/micro-llama-1b"
print("ğŸ”¹ Loading tokenizer and config ...")

tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
config = MiCRoLlamaConfig.from_pretrained(MODEL_ID)

print("ğŸ”¹ Loading model weights (this may take several minutes) ...")
model = MiCRoLlama.from_pretrained(
    MODEL_ID,
    torch_dtype=torch.float16,
    device_map="auto"
)
model.eval()

# ================== ØªØ³Øª ÙˆØ±ÙˆØ¯ÛŒ Ùˆ ØªÙˆÙ„ÛŒØ¯ Ø®Ø±ÙˆØ¬ÛŒ ==================
input_text = "Explain the role of reasoning modules in brain-like language models."
print(f"\nğŸ§  Input: {input_text}\n")

inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=120,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
    )

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("ğŸ§© Model output:\n")
print(generated_text)
